diff --git a/test_lx/DMC/dmc_test.py b/test_lx/DMC/dmc_test.py
index 7a38e75..306959c 100644
--- a/test_lx/DMC/dmc_test.py
+++ b/test_lx/DMC/dmc_test.py
@@ -1,49 +1,103 @@
-import gymnasium as gym
+import os
+import numpy as np
+import mujoco
+import dm_control.suite as suite
+import wandb
+from wandb.integration.sb3 import WandbCallback
 from stable_baselines3 import SAC
-import time
-
-# 1. 创建环境
-# render_mode="human" 会打开一个窗口让你实时看到狗的行为
-# dmc_proprio（本体感受）模式下，输入是关节角度等数值而非图片，训练极快
-env = gym.make("shimmy/dog-walk-v0", render_mode="human")
-
-print(f"动作空间: {env.action_space}") # 应该是 Box(-1.0, 1.0, (38,), float32)
-print(f"观测空间: {env.observation_space}")
-
-# 2. 定义 SAC 模型
-# MlpPolicy 表示使用多层感知机（处理数值状态）
-model = SAC(
-    "MlpPolicy", 
-    env, 
-    verbose=1, 
-    learning_rate=3e-4,
-    buffer_size=100000,
-    batch_size=256,
-    tau=0.005,
-    gamma=0.99,
-    device="cuda" # 如果有GPU请设为cuda，否则cpu
-)
-
-# 3. 开始训练并实时渲染
-print("开始训练... 你应该能看到弹出的 MuJoCo 窗口了。")
-try:
-    # 训练 100,000 步
-    model.learn(total_timesteps=100000, log_interval=10)
-    # 保存模型
-    model.save("sac_dog_walk")
-    print("训练完成并已保存模型。")
-except KeyboardInterrupt:
-    print("训练被用户中断。")
-
-# 4. 训练结束后进行演示
-print("开始演示训练结果...")
-obs, _ = env.reset()
-for _ in range(1000):
-    action, _states = model.predict(obs, deterministic=True)
-    obs, reward, terminated, truncated, info = env.step(action)
-    env.render() # 持续渲染
-    if terminated or truncated:
-        obs, _ = env.reset()
-    time.sleep(0.01)
-
-env.close()
\ No newline at end of file
+from stable_baselines3.common.env_checker import check_env
+import gymnasium as gym
+from gymnasium import spaces
+
+# --- 1. 将 DMC 包装为 Gymnasium 兼容的标准环境 ---
+class DMControlWrapper(gym.Env):
+    def __init__(self, domain_name, task_name, render_mode="rgb_array"):
+        super().__init__()
+        self.env = suite.load(domain_name, task_name)
+        self.render_mode = render_mode
+        
+        # 提取动作空间 (Dog Walk 是 38 维)
+        spec = self.env.action_spec()
+        self.action_space = spaces.Box(low=spec.minimum, high=spec.maximum, dtype=np.float32)
+        
+        # 简单处理：将字典观测值拼接成一个长向量
+        obs_spec = self.env.observation_spec()
+        total_dim = sum([np.prod(v.shape) for v in obs_spec.values()])
+        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(total_dim,), dtype=np.float32)
+
+    def _get_obs(self, time_step):
+        # 拼接字典中的所有观测项
+        return np.concatenate([v.flatten() for v in time_step.observation.values()])
+
+    def reset(self, seed=None, options=None):
+        super().reset(seed=seed)
+        time_step = self.env.reset()
+        return self._get_obs(time_step), {}
+
+    def step(self, action):
+        time_step = self.env.step(action)
+        obs = self._get_obs(time_step)
+        reward = time_step.reward or 0.0
+        terminated = time_step.last()
+        truncated = False
+        return obs, reward, terminated, truncated, {}
+
+    def render(self):
+        # 渲染 Dog 的画面
+        return self.env.physics.render(height=240, width=320, camera_id=0)
+
+# --- 2. 训练与 WandB 可视化 ---
+def train_dog():
+    # 初始化 WandB
+    run = wandb.init(
+        project="dog-walk-baseline",
+        name="sac_dog_walk_v1",
+        sync_tensorboard=True,  # 自动同步 SB3 的日志
+        monitor_gym=True,       # 自动记录训练视频
+        save_code=True,
+    )
+
+    # 创建环境
+    env = DMControlWrapper("dog", "walk")
+
+    # 定义模型 (SAC)
+    # Dog 任务动作维度高达 38，建议增大网络规模
+    model = SAC(
+        "MlpPolicy", 
+        env, 
+        verbose=1, 
+        tensorboard_log=f"runs/{run.id}",
+        learning_rate=3e-4,
+        buffer_size=100000,
+        batch_size=256,
+        tau=0.005,
+        gamma=0.99,
+    )
+
+    # 自定义实时渲染与上传 WandB 的逻辑 (每 5000 步上传一张图片)
+    class VideoLogCallback(WandbCallback):
+        def _on_step(self) -> bool:
+            if self.n_calls % 5000 == 0:
+                img = self.training_env.render()
+                wandb.log({"live_render": wandb.Image(img)}, step=self.num_timesteps)
+            return True
+
+    # 开始训练
+    print("开始训练 Dog Walk 任务...")
+    model.learn(
+        total_timesteps=500000, 
+        callback=VideoLogCallback(
+            gradient_save_freq=100,
+            model_save_path=f"models/{run.id}",
+            verbose=2,
+        )
+    )
+
+    # 保存最终模型
+    model.save("sac_dog_walk_final")
+    run.finish()
+
+if __name__ == "__main__":
+    # 如果在无显示器服务器运行，请确保设置 EGL
+    os.environ['MUJOCO_GL'] = 'egl'
+    train_dog()
\ No newline at end of file
diff --git a/train_dmc.py b/train_dmc.py
index 8ea47ad..7c0f41b 100644
--- a/train_dmc.py
+++ b/train_dmc.py
@@ -1,262 +1,83 @@
-import warnings
-
-warnings.filterwarnings('ignore', category=DeprecationWarning)
-
 import os
-
-os.environ['MKL_SERVICE_FORCE_INTEL'] = '1'
-os.environ['MUJOCO_GL'] = 'egl'
-
-from pathlib import Path
-
-import hydra
+import gymnasium as gym
 import numpy as np
-import utils
-import torch
-from dm_env import specs
-
-import dmc
-
-from logger import Logger
-from replay_buffer import ReplayBufferStorage, make_replay_loader
-from video import TrainVideoRecorder, VideoRecorder
+from dm_control import suite
+from shimmy.dm_control_compatibility import DmControlCompatibilityV0
+from stable_baselines3 import SAC
+from stable_baselines3.common.callbacks import BaseCallback
 import wandb
-import re
-
-from utils import models_tuple
-from copy import deepcopy
-
-torch.backends.cudnn.benchmark = True
-
-
-def make_agent(obs_spec, action_spec, cfg):
-    cfg.obs_shape = obs_spec.shape
-    cfg.action_shape = action_spec.shape
-    return hydra.utils.instantiate(cfg)
-
-
-class Workspace:
-    def __init__(self, cfg):
-        self.work_dir = Path.cwd()
-        self.cfg = cfg
-        print("#"*20)
-        print(f'\nworkspace: {self.work_dir}')
-        print(self.cfg)
-        self.last_save_step = -9999
-        if self.cfg.use_wandb:
-            exp_name = '_'.join([cfg.task_name, str(cfg.seed)])
-            group_name = re.search(r'\.(.+)\.', cfg.agent._target_).group(1)
-            name_1 = cfg.task_name
-            name_2 = group_name
-            try:
-                name_2 += '_' + cfg.title
-            except:
-                pass
-            name_3 = exp_name
-            wandb.init(project=name_1,
-                       group=name_2,
-                       name=name_3,
-                       config=cfg)
-        utils.set_seed_everywhere(cfg.seed)
-        self.device = torch.device(cfg.device)
-        self._discount = cfg.discount
-        self._nstep = cfg.nstep
-        self.setup()
-        self.agent = make_agent(self.train_env.observation_spec(),
-                                self.train_env.action_spec(), self.cfg.agent)
-        self.timer = utils.Timer()
-        self._global_step = 0
-        self._global_episode = 0
-
-    def setup(self):
-        # create logger
-        self.logger = Logger(self.work_dir,
-                             use_tb=self.cfg.use_tb,
-                             use_wandb=self.cfg.use_wandb)
-        # create envs
-        self.train_env = dmc.make(self.cfg.task_name, self.cfg.frame_stack,
-                                  self.cfg.action_repeat, self.cfg.seed)
-        self.eval_env = dmc.make(self.cfg.task_name, self.cfg.frame_stack,
-                                 self.cfg.action_repeat, self.cfg.seed)
-        # create replay buffer
-        data_specs = (self.train_env.observation_spec(),
-                      self.train_env.action_spec(),
-                      specs.Array((1, ), np.float32, 'reward'),
-                      specs.Array((1, ), np.float32, 'discount'))
-
-        self.replay_storage = ReplayBufferStorage(data_specs,
-                                                  self.work_dir / 'buffer')
-        self.replay_loader, self.buffer = make_replay_loader(
-            self.work_dir / 'buffer', self.cfg.replay_buffer_size,
-            self.cfg.batch_size,
-            self.cfg.replay_buffer_num_workers, self.cfg.save_snapshot,
-            self._nstep,
-            self._discount)
-        self._replay_iter = None
-
-        self.video_recorder = VideoRecorder(
-            self.work_dir if self.cfg.save_video else None)
-
-    @property
-    def global_step(self):
-        return self._global_step
-
-    @property
-    def global_episode(self):
-        return self._global_episode
-
-    @property
-    def global_frame(self):
-        return self.global_step * self.cfg.action_repeat
-
-    @property
-    def replay_iter(self):
-        if self._replay_iter is None:
-            self._replay_iter = iter(self.replay_loader)
-        return self._replay_iter
-
-    def eval(self):
-        step, episode, total_reward = 0, 0, 0
-        eval_until_episode = utils.Until(self.cfg.num_eval_episodes)
-
-        while eval_until_episode(episode):
-            time_step = self.eval_env.reset()
-            self.video_recorder.init(self.eval_env, enabled=(episode == 0))
-            while not time_step.last():
-                with torch.no_grad(), utils.eval_mode(self.agent):
-                    action = self.agent.act(time_step.observation,
-                                            self.global_step,
-                                            eval_mode=True)
-                time_step = self.eval_env.step(action)
-                self.video_recorder.record(self.eval_env)
-                total_reward += time_step.reward
-                step += 1
-
-            episode += 1
-            self.video_recorder.save(f'{self.global_frame}.mp4')
-        with self.logger.log_and_dump_ctx(self.global_frame, ty='eval') as log:
-            log('episode_reward', total_reward / episode)
-            log('episode_length', step * self.cfg.action_repeat / episode)
-            log('episode', self.global_episode)
-            log('step', self.global_step)
-
-    def train(self):
-        # predicates 
-        # frames = steps * action_repeat
-        train_until_step = utils.Until(self.cfg.num_train_frames,
-                                       self.cfg.action_repeat)
-        seed_until_step = utils.Until(self.cfg.num_seed_frames,
-                                      self.cfg.action_repeat)
-        eval_every_step = utils.Every(self.cfg.eval_every_frames,
-                                      self.cfg.action_repeat)
-
-        episode_step, episode_reward = 0, 0
-        time_step = self.train_env.reset()
-        self.replay_storage.add(time_step)
-        metrics = None
-        print("start training")
-        while train_until_step(self.global_step):
-            if time_step.last():
-                self._global_episode += 1
-                # wait until all the metrics schema is populated
-                if metrics is not None:
-                    # log stats
-                    elapsed_time, total_time = self.timer.reset()
-                    episode_frame = episode_step * self.cfg.action_repeat
-                    with self.logger.log_and_dump_ctx(self.global_frame,
-                                                      ty='train') as log:
-                        log('fps', episode_frame / elapsed_time)
-                        log('total_time', total_time)
-                        log('episode_reward', episode_reward)
-                        log('episode_length', episode_frame)
-                        log('episode', self.global_episode)
-                        log('buffer_size', len(self.replay_storage))
-                        log('step', self.global_step)
-                # update priority queue
-                if hasattr(self.agent, 'tp_set'):
-                    self.agent.tp_set.add(episode_reward,\
-                                            deepcopy(self.agent.actor),\
-                                            deepcopy(self.agent.critic),\
-                                            deepcopy(self.agent.critic_target),\
-                                            deepcopy(self.agent.value_predictor),\
-                                            moe=deepcopy(self.agent.actor.moe.experts),\
-                                            gate=deepcopy(self.agent.actor.moe.gate))                    
-                # reset env
-                time_step = self.train_env.reset()
-                self.replay_storage.add(time_step)
-                if self.cfg.save_snapshot and self.global_step - self.last_save_step >= self.cfg.save_interval:
-                    self.last_save_step = self.global_step
-                    self.save_snapshot(self.global_step)
-                episode_step = 0
-                episode_reward = 0
-
-            # try to evaluate
-            if eval_every_step(self.global_step):
-                self.logger.log('eval_total_time', self.timer.total_time(),
-                                self.global_frame)
-                self.eval()
-
-            # sample action
-            with torch.no_grad(), utils.eval_mode(self.agent):
-                action = self.agent.act(time_step.observation,
-                                        self.global_step,
-                                        eval_mode=False)
-
-            # try to update the agent
-            if not seed_until_step(self.global_step):
-                metrics = self.agent.update(
-                    self.replay_iter, self.global_step
-                ) if self.global_step % self.cfg.update_every_steps == 0 else dict()
-                if hasattr(self.agent, 'tp_set'):
-                    metrics = self.agent.tp_set.log(metrics)
-                self.logger.log_metrics(metrics, self.global_frame, ty='train')
-
-            # take env step
-            time_step = self.train_env.step(action)
-            episode_reward += time_step.reward
-            self.replay_storage.add(time_step)
-            episode_step += 1
-            self._global_step += 1
-
-    def save_snapshot(self, step_id=None):
-        if step_id is None:
-            snapshot = self.work_dir / 'snapshot.pt'
-        else:
-            if not os.path.exists(str(self.work_dir) + '/snapshots'):
-                os.makedirs(str(self.work_dir) + '/snapshots')
-            snapshot = self.work_dir / 'snapshots' / 'snapshot_{}.pt'.format(step_id)
-        keys_to_save = ['agent', 'timer', '_global_step', '_global_episode']
-        payload = {k: self.__dict__[k] for k in keys_to_save}
-        with snapshot.open('wb') as f:
-            torch.save(payload, f)
-
-    def load_snapshot(self, step_id=None):
-        if step_id is None:
-            snapshot = self.work_dir / 'snapshot.pt'
-        else:
-            snapshot = self.work_dir / 'snapshots' / 'snapshot_{}.pt'.format(step_id)
-        if not snapshot.exists():
-            raise FileNotFoundError(f"Snapshot {snapshot} not found.")
-        with snapshot.open('rb') as f:
-            payload = torch.load(f)
-        for k, v in payload.items():
-            self.__dict__[k] = v
-
-
-@hydra.main(config_path='cfgs', config_name='config')
-def main(cfgs):
-    from train_dmc import Workspace as W
-    root_dir = Path.cwd()
-    workspace = W(cfgs)
-    if cfgs.load_from_id:
-        snapshot = root_dir / 'snapshots' / f'snapshot_{cfgs.load_id}.pt'
-    else:
-        snapshot = root_dir / 'snapshot.pt'
-    if snapshot.exists():
-        print(f'resuming: {snapshot}')
-        workspace.load_snapshot()        
-    workspace.train()
-
-
-if __name__ == '__main__':
-    main()
+from wandb.integration.sb3 import WandbCallback
+
+# 1. 环境配置
+os.environ['MUJOCO_GL'] = 'egl'  # 无头模式渲染
+
+class WandbVideoCallback(BaseCallback):
+    """自定义回调：每隔一段时间渲染一段视频并上传到 WandB"""
+    def __init__(self, eval_env, render_freq=10000):
+        super().__init__()
+        self.eval_env = eval_env
+        self.render_freq = render_freq
+
+    def _on_step(self) -> bool:
+        if self.n_calls % self.render_freq == 0:
+            # 简单录制一个片段
+            screens = []
+            obs, _ = self.eval_env.reset()
+            for _ in range(200):
+                action, _ = self.model.predict(obs, deterministic=True)
+                obs, _, terminated, truncated, _ = self.eval_env.step(action)
+                # 渲染视觉图
+                screens.append(self.eval_env.render().transpose(2, 0, 1))
+                if terminated or truncated: break
+            
+            # 上传到 WandB
+            wandb.log({"video": wandb.Video(np.array(screens), fps=30, format="mp4")})
+        return True
+
+def make_dmc_env():
+    # 使用 shimmy 库将 DMC 转换为 Gymnasium 接口
+    env = suite.load(domain_name="dog", task_name="walk")
+    env = DmControlCompatibilityV0(env, render_mode="rgb_array")
+    env = gym.wrappers.RescaleAction(env, min_action=-1, max_action=1)
+    return env
+
+if __name__ == "__main__":
+    # 2. 初始化 WandB
+    run = wandb.init(
+        project="dog_walk_study",
+        config={"algorithm": "SAC", "total_timesteps": 1000000},
+        sync_tensorboard=True, 
+        monitor_gym=True,
+        save_code=True,
+    )
+
+    # 3. 创建环境
+    env = make_dmc_env()
+
+    # 4. 定义模型 (SAC 是处理连续动作空间最成熟的模型之一)
+    model = SAC(
+        "MlpPolicy", 
+        env, 
+        verbose=1, 
+        tensorboard_log=f"runs/{run.id}",
+        learning_rate=3e-4,
+        buffer_size=100000,
+        batch_size=256,
+        tau=0.005,
+        gamma=0.99,
+    )
+
+    # 5. 开始训练并记录视频
+    model.learn(
+        total_timesteps=run.config["total_timesteps"],
+        callback=[
+            WandbCallback(
+                gradient_save_freq=100,
+                model_save_path=f"models/{run.id}",
+                verbose=2,
+            ),
+            WandbVideoCallback(make_dmc_env(), render_freq=5000)
+        ]
+    )
+
+    run.finish()
\ No newline at end of file
